[2026-02-02T10:23:58.804+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T09:46:14.958410+00:00 [queued]>
[2026-02-02T10:23:58.809+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T09:46:14.958410+00:00 [queued]>
[2026-02-02T10:23:58.809+0000] {taskinstance.py:2170} INFO - Starting attempt 7 of 8
[2026-02-02T10:23:58.818+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): run_batch_pipeline> on 2026-02-02 09:46:14.958410+00:00
[2026-02-02T10:23:58.835+0000] {standard_task_runner.py:60} INFO - Started process 2350 to run task
[2026-02-02T10:23:58.838+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'daily_order_analytics', 'run_batch_pipeline', 'manual__2026-02-02T09:46:14.958410+00:00', '--job-id', '59', '--raw', '--subdir', 'DAGS_FOLDER/daily_batch_dag.py', '--cfg-path', '/tmp/tmp26oqxspv']
[2026-02-02T10:23:58.841+0000] {standard_task_runner.py:88} INFO - Job 59: Subtask run_batch_pipeline
[2026-02-02T10:23:58.974+0000] {task_command.py:423} INFO - Running <TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T09:46:14.958410+00:00 [running]> on host 4ba644d6f495
[2026-02-02T10:23:59.077+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-engineering' AIRFLOW_CTX_DAG_ID='daily_order_analytics' AIRFLOW_CTX_TASK_ID='run_batch_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2026-02-02T09:46:14.958410+00:00' AIRFLOW_CTX_TRY_NUMBER='7' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-02-02T09:46:14.958410+00:00'
[2026-02-02T10:23:59.088+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2026-02-02T10:23:59.089+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local --conf spark.jars.packages=org.***ql:***ql:42.7.1 --conf spark.driver.memory=2g --conf spark.executor.memory=2g --name arrow-spark /opt/airflow/dags/src/batch_pipeline.py 2026-02-02
[2026-02-02T10:23:59.270+0000] {spark_submit.py:634} INFO - /home/airflow/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2026-02-02T10:24:02.088+0000] {spark_submit.py:634} INFO - :: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2026-02-02T10:24:02.498+0000] {spark_submit.py:634} INFO - Ivy Default Cache set to: /home/airflow/.ivy2/cache
[2026-02-02T10:24:02.499+0000] {spark_submit.py:634} INFO - The jars for the packages stored in: /home/airflow/.ivy2/jars
[2026-02-02T10:24:02.501+0000] {spark_submit.py:634} INFO - org.***ql#***ql added as a dependency
[2026-02-02T10:24:02.502+0000] {spark_submit.py:634} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-e578a192-2eb1-42bd-9513-cd59482759a3;1.0
[2026-02-02T10:24:02.502+0000] {spark_submit.py:634} INFO - confs: [default]
[2026-02-02T10:24:02.901+0000] {spark_submit.py:634} INFO - found org.***ql#***ql;42.7.1 in central
[2026-02-02T10:24:03.003+0000] {spark_submit.py:634} INFO - found org.checkerframework#checker-qual;3.41.0 in central
[2026-02-02T10:24:03.038+0000] {spark_submit.py:634} INFO - :: resolution report :: resolve 523ms :: artifacts dl 13ms
[2026-02-02T10:24:03.039+0000] {spark_submit.py:634} INFO - :: modules in use:
[2026-02-02T10:24:03.040+0000] {spark_submit.py:634} INFO - org.checkerframework#checker-qual;3.41.0 from central in [default]
[2026-02-02T10:24:03.041+0000] {spark_submit.py:634} INFO - org.***ql#***ql;42.7.1 from central in [default]
[2026-02-02T10:24:03.041+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:24:03.043+0000] {spark_submit.py:634} INFO - |                  |            modules            ||   artifacts   |
[2026-02-02T10:24:03.043+0000] {spark_submit.py:634} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2026-02-02T10:24:03.044+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:24:03.044+0000] {spark_submit.py:634} INFO - |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2026-02-02T10:24:03.045+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:24:03.046+0000] {spark_submit.py:634} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-e578a192-2eb1-42bd-9513-cd59482759a3
[2026-02-02T10:24:03.047+0000] {spark_submit.py:634} INFO - confs: [default]
[2026-02-02T10:24:03.057+0000] {spark_submit.py:634} INFO - 0 artifacts copied, 2 already retrieved (0kB/11ms)
[2026-02-02T10:24:03.504+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-02-02T10:24:04.917+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO SparkContext: Running Spark version 3.5.0
[2026-02-02T10:24:04.921+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO SparkContext: OS info Linux, 6.12.65-linuxkit, aarch64
[2026-02-02T10:24:04.921+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO SparkContext: Java version 17.0.18
[2026-02-02T10:24:04.952+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceUtils: ==============================================================
[2026-02-02T10:24:04.954+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2026-02-02T10:24:04.955+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceUtils: ==============================================================
[2026-02-02T10:24:04.955+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO SparkContext: Submitted application: Daily_ETL_2026-02-02
[2026-02-02T10:24:04.979+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2026-02-02T10:24:04.986+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceProfile: Limiting resource is cpu
[2026-02-02T10:24:04.987+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2026-02-02T10:24:05.103+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SecurityManager: Changing view acls to: default
[2026-02-02T10:24:05.104+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SecurityManager: Changing modify acls to: default
[2026-02-02T10:24:05.104+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SecurityManager: Changing view acls groups to:
[2026-02-02T10:24:05.105+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SecurityManager: Changing modify acls groups to:
[2026-02-02T10:24:05.105+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2026-02-02T10:24:05.577+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO Utils: Successfully started service 'sparkDriver' on port 44761.
[2026-02-02T10:24:05.698+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SparkEnv: Registering MapOutputTracker
[2026-02-02T10:24:05.756+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SparkEnv: Registering BlockManagerMaster
[2026-02-02T10:24:05.820+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2026-02-02T10:24:05.821+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2026-02-02T10:24:05.826+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2026-02-02T10:24:05.859+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b5ae0922-67e9-43c0-baf4-c5a6adb8271b
[2026-02-02T10:24:05.875+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
[2026-02-02T10:24:05.894+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2026-02-02T10:24:06.045+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2026-02-02T10:24:06.136+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2026-02-02T10:24:06.164+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar at spark://4ba644d6f495:44761/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027844902
[2026-02-02T10:24:06.165+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar at spark://4ba644d6f495:44761/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027844902
[2026-02-02T10:24:06.168+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar at file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027844902
[2026-02-02T10:24:06.170+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Copying /home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:24:06.213+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar at file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027844902
[2026-02-02T10:24:06.215+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Copying /home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:24:06.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Starting executor ID driver on host 4ba644d6f495
[2026-02-02T10:24:06.325+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: OS info Linux, 6.12.65-linuxkit, aarch64
[2026-02-02T10:24:06.325+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Java version 17.0.18
[2026-02-02T10:24:06.329+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2026-02-02T10:24:06.330+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1677b591 for default.
[2026-02-02T10:24:06.341+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027844902
[2026-02-02T10:24:06.385+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: /home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar has been previously copied to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:24:06.400+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027844902
[2026-02-02T10:24:06.402+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: /home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar has been previously copied to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:24:06.424+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Fetching spark://4ba644d6f495:44761/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027844902
[2026-02-02T10:24:06.487+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO TransportClientFactory: Successfully created connection to 4ba644d6f495/172.18.0.4:44761 after 39 ms (0 ms spent in bootstraps)
[2026-02-02T10:24:06.491+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Fetching spark://4ba644d6f495:44761/jars/org.checkerframework_checker-qual-3.41.0.jar to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/fetchFileTemp15443957962444258839.tmp
[2026-02-02T10:24:06.539+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/fetchFileTemp15443957962444258839.tmp has been previously copied to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:24:06.549+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Adding file:/tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.checkerframework_checker-qual-3.41.0.jar to class loader default
[2026-02-02T10:24:06.550+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Fetching spark://4ba644d6f495:44761/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027844902
[2026-02-02T10:24:06.552+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Fetching spark://4ba644d6f495:44761/jars/org.***ql_***ql-42.7.1.jar to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/fetchFileTemp10908548670981387862.tmp
[2026-02-02T10:24:06.576+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/fetchFileTemp10908548670981387862.tmp has been previously copied to /tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:24:06.581+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Executor: Adding file:/tmp/spark-2e3bc0b1-dd0b-43ed-b8e2-6fd7dbe74a3f/userFiles-12e12b26-9a10-4fcf-8a2d-b96a8c5b65da/org.***ql_***ql-42.7.1.jar to class loader default
[2026-02-02T10:24:06.592+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36915.
[2026-02-02T10:24:06.593+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO NettyBlockTransferService: Server created on 4ba644d6f495:36915
[2026-02-02T10:24:06.594+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2026-02-02T10:24:06.601+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ba644d6f495, 36915, None)
[2026-02-02T10:24:06.607+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO BlockManagerMasterEndpoint: Registering block manager 4ba644d6f495:36915 with 1048.8 MiB RAM, BlockManagerId(driver, 4ba644d6f495, 36915, None)
[2026-02-02T10:24:06.631+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ba644d6f495, 36915, None)
[2026-02-02T10:24:06.632+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ba644d6f495, 36915, None)
[2026-02-02T10:24:07.087+0000] {spark_submit.py:634} INFO - DEBUG: Processing data for 2026-02-02
[2026-02-02T10:24:07.096+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2026-02-02T10:24:07.102+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:07 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2026-02-02T10:24:09.379+0000] {spark_submit.py:634} INFO - Loading data to PostgreSQL...
[2026-02-02T10:24:10.461+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO CodeGenerator: Code generated in 154.384083 ms
[2026-02-02T10:24:10.646+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Registering RDD 3 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2026-02-02T10:24:10.653+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO CodeGenerator: Code generated in 22.206 ms
[2026-02-02T10:24:10.670+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Got map stage job 0 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:10.672+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:10.676+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:10.677+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:10.678+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:11.000+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.5 KiB, free 1048.8 MiB)
[2026-02-02T10:24:11.069+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1048.8 MiB)
[2026-02-02T10:24:11.073+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4ba644d6f495:36915 (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:11.076+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:11.147+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:11.151+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2026-02-02T10:24:11.218+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Registering RDD 5 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2026-02-02T10:24:11.219+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Got map stage job 1 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:11.219+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:11.220+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:11.220+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:11.222+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:11.230+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 1048.8 MiB)
[2026-02-02T10:24:11.244+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1048.8 MiB)
[2026-02-02T10:24:11.244+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4ba644d6f495:36915 (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:11.245+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:11.245+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:11.246+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2026-02-02T10:24:11.250+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:24:11.268+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2026-02-02T10:24:11.510+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO CodeGenerator: Code generated in 15.855292 ms
[2026-02-02T10:24:11.572+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO CodeGenerator: Code generated in 24.597666 ms
[2026-02-02T10:24:11.879+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO JDBCRDD: closed connection
[2026-02-02T10:24:11.956+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1989 bytes result sent to driver
[2026-02-02T10:24:11.972+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:24:11.974+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2026-02-02T10:24:11.979+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 741 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:11.981+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2026-02-02T10:24:11.998+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: ShuffleMapStage 0 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.278 s
[2026-02-02T10:24:11.999+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:11.999+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2026-02-02T10:24:12.000+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:11 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:12.000+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:12.066+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 22.2895 ms
[2026-02-02T10:24:12.093+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 10.098833 ms
[2026-02-02T10:24:12.118+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO JDBCRDD: closed connection
[2026-02-02T10:24:12.128+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1989 bytes result sent to driver
[2026-02-02T10:24:12.130+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 159 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:12.130+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2026-02-02T10:24:12.132+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.907 s
[2026-02-02T10:24:12.132+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:12.133+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: running: Set()
[2026-02-02T10:24:12.133+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:12.133+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:12.169+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO ShufflePartitionsUtil: For shuffle(0, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:24:12.258+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4ba644d6f495:36915 in memory (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:12.279+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4ba644d6f495:36915 in memory (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:12.368+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 70.144708 ms
[2026-02-02T10:24:12.381+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 7.413041 ms
[2026-02-02T10:24:12.427+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 6.235333 ms
[2026-02-02T10:24:12.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Registering RDD 12 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 2
[2026-02-02T10:24:12.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Got map stage job 2 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:12.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:12.516+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
[2026-02-02T10:24:12.517+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:12.519+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:12.543+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 66.3 KiB, free 1048.7 MiB)
[2026-02-02T10:24:12.560+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 1048.7 MiB)
[2026-02-02T10:24:12.561+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4ba644d6f495:36915 (size: 29.3 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:12.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:12.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:12.563+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2026-02-02T10:24:12.569+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 2) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8379 bytes)
[2026-02-02T10:24:12.574+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO Executor: Running task 0.0 in stage 4.0 (TID 2)
[2026-02-02T10:24:12.662+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO ShuffleBlockFetcherIterator: Getting 1 (748.2 KiB) non-empty blocks including 1 (748.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:12.668+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2026-02-02T10:24:12.711+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 29.69075 ms
[2026-02-02T10:24:12.767+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 23.287292 ms
[2026-02-02T10:24:12.795+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 13.765167 ms
[2026-02-02T10:24:12.820+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO ShuffleBlockFetcherIterator: Getting 1 (970.0 B) non-empty blocks including 1 (970.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:12.820+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2026-02-02T10:24:12.835+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 14.00225 ms
[2026-02-02T10:24:12.845+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 6.959916 ms
[2026-02-02T10:24:12.865+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 14.201542 ms
[2026-02-02T10:24:12.913+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 46.104709 ms
[2026-02-02T10:24:12.940+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 8.906459 ms
[2026-02-02T10:24:12.952+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:12 INFO CodeGenerator: Code generated in 5.138375 ms
[2026-02-02T10:24:13.429+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:13 INFO CodeGenerator: Code generated in 123.031875 ms
[2026-02-02T10:24:13.628+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:13 INFO CodeGenerator: Code generated in 53.882542 ms
[2026-02-02T10:24:15.301+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 2). 6897 bytes result sent to driver
[2026-02-02T10:24:15.312+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 2) in 2744 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:15.313+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2026-02-02T10:24:15.321+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 2.782 s
[2026-02-02T10:24:15.322+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:15.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: running: Set()
[2026-02-02T10:24:15.324+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:15.324+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:15.350+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:24:15.382+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2026-02-02T10:24:15.443+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO CodeGenerator: Code generated in 35.40375 ms
[2026-02-02T10:24:15.534+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0
[2026-02-02T10:24:15.538+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Got job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:15.539+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Final stage: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:15.540+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2026-02-02T10:24:15.541+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:15.544+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:15.555+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 70.2 KiB, free 1048.6 MiB)
[2026-02-02T10:24:15.559+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.6 KiB, free 1048.6 MiB)
[2026-02-02T10:24:15.560+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4ba644d6f495:36915 (size: 30.6 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:15.560+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:15.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:15.563+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2026-02-02T10:24:15.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 3) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8108 bytes)
[2026-02-02T10:24:15.568+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO Executor: Running task 0.0 in stage 8.0 (TID 3)
[2026-02-02T10:24:15.600+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO ShuffleBlockFetcherIterator: Getting 1 (1600.0 B) non-empty blocks including 1 (1600.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:15.601+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2026-02-02T10:24:15.665+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO CodeGenerator: Code generated in 63.414875 ms
[2026-02-02T10:24:15.752+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:15 INFO CodeGenerator: Code generated in 32.331042 ms
[2026-02-02T10:24:16.127+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 3). 7900 bytes result sent to driver
[2026-02-02T10:24:16.134+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 3) in 568 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:16.135+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2026-02-02T10:24:16.137+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.589 s
[2026-02-02T10:24:16.140+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-02T10:24:16.141+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2026-02-02T10:24:16.142+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Job 3 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.607723 s
[2026-02-02T10:24:16.338+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Registering RDD 21 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 3
[2026-02-02T10:24:16.340+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Got map stage job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:16.340+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:16.340+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:16.341+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:16.341+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:16.342+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.5 KiB, free 1048.6 MiB)
[2026-02-02T10:24:16.353+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1048.6 MiB)
[2026-02-02T10:24:16.354+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4ba644d6f495:36915 (size: 7.9 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:16.355+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:16.356+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:16.359+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO CodeGenerator: Code generated in 16.449041 ms
[2026-02-02T10:24:16.360+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2026-02-02T10:24:16.360+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 4) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:24:16.361+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO Executor: Running task 0.0 in stage 9.0 (TID 4)
[2026-02-02T10:24:16.362+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Registering RDD 23 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 4
[2026-02-02T10:24:16.364+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Got map stage job 5 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:16.365+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:16.365+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:16.365+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:16.366+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:16.366+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.7 KiB, free 1048.6 MiB)
[2026-02-02T10:24:16.371+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1048.6 MiB)
[2026-02-02T10:24:16.374+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4ba644d6f495:36915 (size: 7.6 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:16.374+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:16.375+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:16.375+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2026-02-02T10:24:16.685+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO JDBCRDD: closed connection
[2026-02-02T10:24:16.718+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO Executor: Finished task 0.0 in stage 9.0 (TID 4). 1989 bytes result sent to driver
[2026-02-02T10:24:16.728+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 5) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:24:16.733+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO Executor: Running task 0.0 in stage 10.0 (TID 5)
[2026-02-02T10:24:16.736+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 4) in 377 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:16.737+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2026-02-02T10:24:16.750+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: ShuffleMapStage 9 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.401 s
[2026-02-02T10:24:16.751+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:16.752+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: running: Set(ShuffleMapStage 10)
[2026-02-02T10:24:16.752+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:16.752+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:16.813+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO CodeGenerator: Code generated in 18.189125 ms
[2026-02-02T10:24:16.837+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO JDBCRDD: closed connection
[2026-02-02T10:24:16.841+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO Executor: Finished task 0.0 in stage 10.0 (TID 5). 1989 bytes result sent to driver
[2026-02-02T10:24:16.842+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 5) in 117 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:16.842+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2026-02-02T10:24:16.843+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.480 s
[2026-02-02T10:24:16.844+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:16.845+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: running: Set()
[2026-02-02T10:24:16.846+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:16.848+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:16.860+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO ShufflePartitionsUtil: For shuffle(3, 4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:24:16.981+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:16 INFO CodeGenerator: Code generated in 63.381834 ms
[2026-02-02T10:24:17.019+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Registering RDD 30 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 5
[2026-02-02T10:24:17.020+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Got map stage job 6 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:17.020+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:17.021+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12, ShuffleMapStage 11)
[2026-02-02T10:24:17.021+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:17.021+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[30] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:17.028+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 67.9 KiB, free 1048.5 MiB)
[2026-02-02T10:24:17.050+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1048.5 MiB)
[2026-02-02T10:24:17.054+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4ba644d6f495:36915 (size: 30.1 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:17.055+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:17.055+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[30] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:17.057+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2026-02-02T10:24:17.058+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 6) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8379 bytes)
[2026-02-02T10:24:17.061+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO Executor: Running task 0.0 in stage 13.0 (TID 6)
[2026-02-02T10:24:17.091+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Getting 1 (748.2 KiB) non-empty blocks including 1 (748.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:17.092+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2026-02-02T10:24:17.097+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Getting 1 (1082.0 B) non-empty blocks including 1 (1082.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:17.099+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2026-02-02T10:24:17.203+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 96.578792 ms
[2026-02-02T10:24:17.219+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 10.393333 ms
[2026-02-02T10:24:17.225+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 3.753958 ms
[2026-02-02T10:24:17.234+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 4.692584 ms
[2026-02-02T10:24:17.244+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 5.419417 ms
[2026-02-02T10:24:17.475+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4ba644d6f495:36915 in memory (size: 7.9 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:17.488+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4ba644d6f495:36915 in memory (size: 7.6 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:17.506+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4ba644d6f495:36915 in memory (size: 30.6 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:17.602+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO Executor: Finished task 0.0 in stage 13.0 (TID 6). 6897 bytes result sent to driver
[2026-02-02T10:24:17.633+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 6) in 574 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:17.635+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2026-02-02T10:24:17.642+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.616 s
[2026-02-02T10:24:17.644+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:24:17.645+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: running: Set()
[2026-02-02T10:24:17.645+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:24:17.645+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: failed: Set()
[2026-02-02T10:24:17.688+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:24:17.707+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2026-02-02T10:24:17.808+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 25.3 ms
[2026-02-02T10:24:17.873+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0
[2026-02-02T10:24:17.876+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Got job 7 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:17.876+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Final stage: ResultStage 17 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:17.877+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2026-02-02T10:24:17.877+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:17.878+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:17.893+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 69.1 KiB, free 1048.5 MiB)
[2026-02-02T10:24:17.910+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1048.5 MiB)
[2026-02-02T10:24:17.915+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4ba644d6f495:36915 (size: 30.1 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:17.918+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:17.921+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:17.922+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2026-02-02T10:24:17.923+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 7) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8108 bytes)
[2026-02-02T10:24:17.923+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO Executor: Running task 0.0 in stage 17.0 (TID 7)
[2026-02-02T10:24:17.938+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Getting 1 (6.5 KiB) non-empty blocks including 1 (6.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:24:17.939+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2026-02-02T10:24:17.986+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO CodeGenerator: Code generated in 44.297625 ms
[2026-02-02T10:24:17.988+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4ba644d6f495:36915 in memory (size: 29.3 KiB, free: 1048.7 MiB)
[2026-02-02T10:24:18.037+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:18 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 4ba644d6f495:36915 in memory (size: 30.1 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:19.302+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO CodeGenerator: Code generated in 23.254375 ms
[2026-02-02T10:24:19.455+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO Executor: Finished task 0.0 in stage 17.0 (TID 7). 7900 bytes result sent to driver
[2026-02-02T10:24:19.469+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 7) in 1553 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:24:19.472+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2026-02-02T10:24:19.483+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO DAGScheduler: ResultStage 17 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.593 s
[2026-02-02T10:24:19.484+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-02T10:24:19.485+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2026-02-02T10:24:19.486+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:19 INFO DAGScheduler: Job 7 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.611540 s
[2026-02-02T10:24:20.200+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO CodeGenerator: Code generated in 76.701584 ms
[2026-02-02T10:24:20.251+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Registering RDD 40 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 6
[2026-02-02T10:24:20.256+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Got map stage job 8 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:20.258+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:20.261+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:20.281+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:20.291+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[40] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:20.436+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 28.3 KiB, free 1048.7 MiB)
[2026-02-02T10:24:20.454+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1048.7 MiB)
[2026-02-02T10:24:20.460+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4ba644d6f495:36915 (size: 13.6 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:20.461+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:20.462+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[40] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:20.464+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2026-02-02T10:24:20.467+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Registering RDD 42 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 7
[2026-02-02T10:24:20.468+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Got map stage job 9 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:24:20.470+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:24:20.471+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:24:20.474+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:24:20.475+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[42] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:24:20.476+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 8) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:24:20.479+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.3 KiB, free 1048.6 MiB)
[2026-02-02T10:24:20.480+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO Executor: Running task 0.0 in stage 18.0 (TID 8)
[2026-02-02T10:24:20.492+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1048.6 MiB)
[2026-02-02T10:24:20.494+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4ba644d6f495:36915 (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:20.496+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:24:20.498+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[42] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:24:20.499+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:20 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2026-02-02T10:24:21.995+0000] {spark_submit.py:634} INFO - 26/02/02 10:24:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 4ba644d6f495:36915 in memory (size: 30.1 KiB, free: 1048.8 MiB)
[2026-02-02T10:24:39.744+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 176, in execute
    self._hook.submit(self.application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 560, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --conf spark.jars.packages=org.***ql:***ql:42.7.1 --conf spark.driver.memory=2g --conf spark.executor.memory=2g --name arrow-spark /opt/airflow/dags/src/batch_pipeline.py 2026-02-02. Error code is: -9.
[2026-02-02T10:24:39.881+0000] {taskinstance.py:1138} INFO - Marking task as UP_FOR_RETRY. dag_id=daily_order_analytics, task_id=run_batch_pipeline, execution_date=20260202T094614, start_date=20260202T102358, end_date=20260202T102439
[2026-02-02T10:24:39.951+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 59 for task run_batch_pipeline (Cannot execute: spark-submit --master local --conf spark.jars.packages=org.***ql:***ql:42.7.1 --conf spark.driver.memory=2g --conf spark.executor.memory=2g --name arrow-spark /opt/airflow/dags/src/batch_pipeline.py 2026-02-02. Error code is: -9.; 2350)
[2026-02-02T10:24:40.031+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2026-02-02T10:24:40.608+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
