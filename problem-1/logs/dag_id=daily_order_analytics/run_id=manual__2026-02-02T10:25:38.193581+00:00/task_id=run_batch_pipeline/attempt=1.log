[2026-02-02T10:25:57.998+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T10:25:38.193581+00:00 [queued]>
[2026-02-02T10:25:58.002+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T10:25:38.193581+00:00 [queued]>
[2026-02-02T10:25:58.002+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 3
[2026-02-02T10:25:58.008+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): run_batch_pipeline> on 2026-02-02 10:25:38.193581+00:00
[2026-02-02T10:25:58.021+0000] {standard_task_runner.py:60} INFO - Started process 227 to run task
[2026-02-02T10:25:58.028+0000] {standard_task_runner.py:87} INFO - Running: ['airflow', 'tasks', 'run', 'daily_order_analytics', 'run_batch_pipeline', 'manual__2026-02-02T10:25:38.193581+00:00', '--job-id', '63', '--raw', '--subdir', 'DAGS_FOLDER/daily_batch_dag.py', '--cfg-path', '/tmp/tmpayuk_u4u']
[2026-02-02T10:25:58.032+0000] {standard_task_runner.py:88} INFO - Job 63: Subtask run_batch_pipeline
[2026-02-02T10:25:58.081+0000] {task_command.py:423} INFO - Running <TaskInstance: daily_order_analytics.run_batch_pipeline manual__2026-02-02T10:25:38.193581+00:00 [running]> on host 4ba644d6f495
[2026-02-02T10:25:58.235+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='data-engineering' AIRFLOW_CTX_DAG_ID='daily_order_analytics' AIRFLOW_CTX_TASK_ID='run_batch_pipeline' AIRFLOW_CTX_EXECUTION_DATE='2026-02-02T10:25:38.193581+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2026-02-02T10:25:38.193581+00:00'
[2026-02-02T10:25:58.287+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2026-02-02T10:25:58.291+0000] {spark_submit.py:473} INFO - Spark-Submit cmd: spark-submit --master local --conf spark.jars.packages=org.***ql:***ql:42.7.1 --conf spark.driver.memory=2g --conf spark.executor.memory=2g --name arrow-spark /opt/airflow/dags/src/batch_pipeline.py 2026-02-02
[2026-02-02T10:25:58.532+0000] {spark_submit.py:634} INFO - /home/airflow/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2026-02-02T10:26:01.948+0000] {spark_submit.py:634} INFO - :: loading settings :: url = jar:file:/home/airflow/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2026-02-02T10:26:02.045+0000] {spark_submit.py:634} INFO - Ivy Default Cache set to: /home/airflow/.ivy2/cache
[2026-02-02T10:26:02.046+0000] {spark_submit.py:634} INFO - The jars for the packages stored in: /home/airflow/.ivy2/jars
[2026-02-02T10:26:02.050+0000] {spark_submit.py:634} INFO - org.***ql#***ql added as a dependency
[2026-02-02T10:26:02.050+0000] {spark_submit.py:634} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-ba528780-4158-4b25-962e-d470d12387ce;1.0
[2026-02-02T10:26:02.050+0000] {spark_submit.py:634} INFO - confs: [default]
[2026-02-02T10:26:02.228+0000] {spark_submit.py:634} INFO - found org.***ql#***ql;42.7.1 in central
[2026-02-02T10:26:02.252+0000] {spark_submit.py:634} INFO - found org.checkerframework#checker-qual;3.41.0 in central
[2026-02-02T10:26:02.269+0000] {spark_submit.py:634} INFO - :: resolution report :: resolve 211ms :: artifacts dl 8ms
[2026-02-02T10:26:02.269+0000] {spark_submit.py:634} INFO - :: modules in use:
[2026-02-02T10:26:02.269+0000] {spark_submit.py:634} INFO - org.checkerframework#checker-qual;3.41.0 from central in [default]
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - org.***ql#***ql;42.7.1 from central in [default]
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - |                  |            modules            ||   artifacts   |
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2026-02-02T10:26:02.270+0000] {spark_submit.py:634} INFO - ---------------------------------------------------------------------
[2026-02-02T10:26:02.272+0000] {spark_submit.py:634} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-ba528780-4158-4b25-962e-d470d12387ce
[2026-02-02T10:26:02.272+0000] {spark_submit.py:634} INFO - confs: [default]
[2026-02-02T10:26:02.276+0000] {spark_submit.py:634} INFO - 0 artifacts copied, 2 already retrieved (0kB/4ms)
[2026-02-02T10:26:02.673+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2026-02-02T10:26:04.419+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SparkContext: Running Spark version 3.5.0
[2026-02-02T10:26:04.427+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SparkContext: OS info Linux, 6.12.65-linuxkit, aarch64
[2026-02-02T10:26:04.428+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SparkContext: Java version 17.0.18
[2026-02-02T10:26:04.461+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceUtils: ==============================================================
[2026-02-02T10:26:04.462+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceUtils: No custom resources configured for spark.driver.
[2026-02-02T10:26:04.462+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceUtils: ==============================================================
[2026-02-02T10:26:04.462+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SparkContext: Submitted application: Daily_ETL_2026-02-02
[2026-02-02T10:26:04.507+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2026-02-02T10:26:04.518+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceProfile: Limiting resource is cpu
[2026-02-02T10:26:04.519+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2026-02-02T10:26:04.606+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SecurityManager: Changing view acls to: default
[2026-02-02T10:26:04.607+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SecurityManager: Changing modify acls to: default
[2026-02-02T10:26:04.607+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SecurityManager: Changing view acls groups to:
[2026-02-02T10:26:04.607+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SecurityManager: Changing modify acls groups to:
[2026-02-02T10:26:04.607+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[2026-02-02T10:26:04.890+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO Utils: Successfully started service 'sparkDriver' on port 45383.
[2026-02-02T10:26:04.932+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:04 INFO SparkEnv: Registering MapOutputTracker
[2026-02-02T10:26:05.002+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkEnv: Registering BlockManagerMaster
[2026-02-02T10:26:05.026+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2026-02-02T10:26:05.026+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2026-02-02T10:26:05.030+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2026-02-02T10:26:05.060+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54643a20-4371-4864-a0be-e7034acba20a
[2026-02-02T10:26:05.071+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB
[2026-02-02T10:26:05.080+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkEnv: Registering OutputCommitCoordinator
[2026-02-02T10:26:05.204+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2026-02-02T10:26:05.291+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2026-02-02T10:26:05.317+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar at spark://4ba644d6f495:45383/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027964406
[2026-02-02T10:26:05.318+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkContext: Added JAR file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar at spark://4ba644d6f495:45383/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027964406
[2026-02-02T10:26:05.319+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar at file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027964406
[2026-02-02T10:26:05.320+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Copying /home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:26:05.398+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO SparkContext: Added file file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar at file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027964406
[2026-02-02T10:26:05.400+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Copying /home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:26:05.481+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Starting executor ID driver on host 4ba644d6f495
[2026-02-02T10:26:05.482+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: OS info Linux, 6.12.65-linuxkit, aarch64
[2026-02-02T10:26:05.482+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Java version 17.0.18
[2026-02-02T10:26:05.486+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2026-02-02T10:26:05.486+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3296b135 for default.
[2026-02-02T10:26:05.501+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027964406
[2026-02-02T10:26:05.521+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: /home/airflow/.ivy2/jars/org.***ql_***ql-42.7.1.jar has been previously copied to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:26:05.531+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Fetching file:///home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027964406
[2026-02-02T10:26:05.531+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: /home/airflow/.ivy2/jars/org.checkerframework_checker-qual-3.41.0.jar has been previously copied to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:26:05.540+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Fetching spark://4ba644d6f495:45383/jars/org.***ql_***ql-42.7.1.jar with timestamp 1770027964406
[2026-02-02T10:26:05.568+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO TransportClientFactory: Successfully created connection to 4ba644d6f495/172.18.0.4:45383 after 19 ms (0 ms spent in bootstraps)
[2026-02-02T10:26:05.574+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Fetching spark://4ba644d6f495:45383/jars/org.***ql_***ql-42.7.1.jar to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/fetchFileTemp3397419773008254747.tmp
[2026-02-02T10:26:05.771+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/fetchFileTemp3397419773008254747.tmp has been previously copied to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.***ql_***ql-42.7.1.jar
[2026-02-02T10:26:05.777+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Adding file:/tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.***ql_***ql-42.7.1.jar to class loader default
[2026-02-02T10:26:05.777+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Fetching spark://4ba644d6f495:45383/jars/org.checkerframework_checker-qual-3.41.0.jar with timestamp 1770027964406
[2026-02-02T10:26:05.778+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Fetching spark://4ba644d6f495:45383/jars/org.checkerframework_checker-qual-3.41.0.jar to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/fetchFileTemp3484266036945353074.tmp
[2026-02-02T10:26:05.779+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/fetchFileTemp3484266036945353074.tmp has been previously copied to /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.checkerframework_checker-qual-3.41.0.jar
[2026-02-02T10:26:05.785+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Executor: Adding file:/tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/userFiles-44d31c1c-3961-46a7-9b0b-d39ed7951c61/org.checkerframework_checker-qual-3.41.0.jar to class loader default
[2026-02-02T10:26:05.793+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37703.
[2026-02-02T10:26:05.794+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO NettyBlockTransferService: Server created on 4ba644d6f495:37703
[2026-02-02T10:26:05.795+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2026-02-02T10:26:05.801+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4ba644d6f495, 37703, None)
[2026-02-02T10:26:05.805+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManagerMasterEndpoint: Registering block manager 4ba644d6f495:37703 with 1048.8 MiB RAM, BlockManagerId(driver, 4ba644d6f495, 37703, None)
[2026-02-02T10:26:05.806+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4ba644d6f495, 37703, None)
[2026-02-02T10:26:05.807+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4ba644d6f495, 37703, None)
[2026-02-02T10:26:06.256+0000] {spark_submit.py:634} INFO - DEBUG: Processing data for 2026-02-02
[2026-02-02T10:26:06.261+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2026-02-02T10:26:06.263+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:06 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2026-02-02T10:26:10.247+0000] {spark_submit.py:634} INFO - Loading data to PostgreSQL...
[2026-02-02T10:26:11.677+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO CodeGenerator: Code generated in 128.836833 ms
[2026-02-02T10:26:11.779+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Registering RDD 3 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2026-02-02T10:26:11.784+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO CodeGenerator: Code generated in 8.566458 ms
[2026-02-02T10:26:11.789+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Got map stage job 0 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:11.789+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:11.790+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:11.790+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:11.792+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:11.953+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 15.5 KiB, free 1048.8 MiB)
[2026-02-02T10:26:12.032+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1048.8 MiB)
[2026-02-02T10:26:12.036+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4ba644d6f495:37703 (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:12.042+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:12.055+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:12.056+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2026-02-02T10:26:12.076+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Registering RDD 5 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2026-02-02T10:26:12.077+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Got map stage job 1 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:12.077+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:12.078+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:12.078+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:12.078+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:12.080+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 1048.8 MiB)
[2026-02-02T10:26:12.090+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1048.8 MiB)
[2026-02-02T10:26:12.091+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4ba644d6f495:37703 (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:12.091+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:12.091+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:12.091+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2026-02-02T10:26:12.102+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:26:12.119+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2026-02-02T10:26:12.455+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO CodeGenerator: Code generated in 14.927166 ms
[2026-02-02T10:26:12.481+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO CodeGenerator: Code generated in 9.852708 ms
[2026-02-02T10:26:12.634+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO JDBCRDD: closed connection
[2026-02-02T10:26:12.686+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2032 bytes result sent to driver
[2026-02-02T10:26:12.690+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:26:12.692+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2026-02-02T10:26:12.696+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 603 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:12.697+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2026-02-02T10:26:12.706+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: ShuffleMapStage 0 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.903 s
[2026-02-02T10:26:12.709+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:12.711+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2026-02-02T10:26:12.713+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:12.714+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:12.736+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO CodeGenerator: Code generated in 8.658709 ms
[2026-02-02T10:26:12.757+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO CodeGenerator: Code generated in 8.365541 ms
[2026-02-02T10:26:12.768+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO JDBCRDD: closed connection
[2026-02-02T10:26:12.780+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2032 bytes result sent to driver
[2026-02-02T10:26:12.784+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 93 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:12.786+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2026-02-02T10:26:12.786+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: ShuffleMapStage 1 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.707 s
[2026-02-02T10:26:12.787+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:12.787+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: running: Set()
[2026-02-02T10:26:12.787+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:12.788+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:12.804+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4ba644d6f495:37703 in memory (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:12.815+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:12 INFO ShufflePartitionsUtil: For shuffle(0, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:26:13.195+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 212.374833 ms
[2026-02-02T10:26:13.273+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 56.970166 ms
[2026-02-02T10:26:13.525+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 60.899584 ms
[2026-02-02T10:26:13.638+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Registering RDD 12 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 2
[2026-02-02T10:26:13.639+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Got map stage job 2 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:13.640+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:13.641+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2, ShuffleMapStage 3)
[2026-02-02T10:26:13.641+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:13.641+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:13.667+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 66.3 KiB, free 1048.7 MiB)
[2026-02-02T10:26:13.677+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 1048.7 MiB)
[2026-02-02T10:26:13.678+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4ba644d6f495:37703 (size: 29.3 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:13.679+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:13.680+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:13.681+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2026-02-02T10:26:13.692+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 2) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8379 bytes)
[2026-02-02T10:26:13.694+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 2)
[2026-02-02T10:26:13.772+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO ShuffleBlockFetcherIterator: Getting 1 (748.2 KiB) non-empty blocks including 1 (748.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:13.776+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2026-02-02T10:26:13.794+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 10.296917 ms
[2026-02-02T10:26:13.824+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 16.004333 ms
[2026-02-02T10:26:13.851+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 17.692167 ms
[2026-02-02T10:26:13.873+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO ShuffleBlockFetcherIterator: Getting 1 (970.0 B) non-empty blocks including 1 (970.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:13.875+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2026-02-02T10:26:13.891+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 15.60625 ms
[2026-02-02T10:26:13.912+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 10.998834 ms
[2026-02-02T10:26:13.921+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 6.084958 ms
[2026-02-02T10:26:13.956+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 34.996 ms
[2026-02-02T10:26:13.969+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 7.036208 ms
[2026-02-02T10:26:13.977+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:13 INFO CodeGenerator: Code generated in 4.689791 ms
[2026-02-02T10:26:14.008+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4ba644d6f495:37703 in memory (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:14.139+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:14 INFO CodeGenerator: Code generated in 6.241833 ms
[2026-02-02T10:26:14.154+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:14 INFO CodeGenerator: Code generated in 4.252375 ms
[2026-02-02T10:26:15.872+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 2). 6897 bytes result sent to driver
[2026-02-02T10:26:15.883+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 2) in 2197 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:15.884+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2026-02-02T10:26:15.889+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO DAGScheduler: ShuffleMapStage 4 (jdbc at NativeMethodAccessorImpl.java:0) finished in 2.230 s
[2026-02-02T10:26:15.892+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:15.895+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO DAGScheduler: running: Set()
[2026-02-02T10:26:15.895+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:15.896+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:15.912+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:26:15.931+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2026-02-02T10:26:15.978+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:15 INFO CodeGenerator: Code generated in 26.7375 ms
[2026-02-02T10:26:16.168+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0
[2026-02-02T10:26:16.225+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Got job 3 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:16.227+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Final stage: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:16.229+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2026-02-02T10:26:16.231+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:16.235+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:16.389+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 70.2 KiB, free 1048.6 MiB)
[2026-02-02T10:26:16.450+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 1048.6 MiB)
[2026-02-02T10:26:16.458+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4ba644d6f495:37703 (size: 30.5 KiB, free: 1048.7 MiB)
[2026-02-02T10:26:16.462+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:16.469+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:16.474+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2026-02-02T10:26:16.484+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 3) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8108 bytes)
[2026-02-02T10:26:16.491+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 3)
[2026-02-02T10:26:16.639+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4ba644d6f495:37703 in memory (size: 29.3 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:16.833+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO ShuffleBlockFetcherIterator: Getting 1 (1466.0 B) non-empty blocks including 1 (1466.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:16.834+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
[2026-02-02T10:26:16.872+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO CodeGenerator: Code generated in 35.951833 ms
[2026-02-02T10:26:16.937+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:16 INFO CodeGenerator: Code generated in 22.716084 ms
[2026-02-02T10:26:17.200+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 3). 7943 bytes result sent to driver
[2026-02-02T10:26:17.215+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 3) in 734 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:17.216+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2026-02-02T10:26:17.221+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: ResultStage 8 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.971 s
[2026-02-02T10:26:17.225+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-02T10:26:17.229+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2026-02-02T10:26:17.230+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Job 3 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.056208 s
[2026-02-02T10:26:17.512+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Registering RDD 21 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 3
[2026-02-02T10:26:17.512+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Got map stage job 4 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:17.513+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:17.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:17.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:17.515+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:17.516+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 15.5 KiB, free 1048.7 MiB)
[2026-02-02T10:26:17.528+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 1048.7 MiB)
[2026-02-02T10:26:17.529+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4ba644d6f495:37703 (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:17.530+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:17.530+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 4ba644d6f495:37703 in memory (size: 30.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:17.531+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[21] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:17.532+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2026-02-02T10:26:17.535+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 4) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:26:17.538+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO CodeGenerator: Code generated in 18.8695 ms
[2026-02-02T10:26:17.539+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO Executor: Running task 0.0 in stage 9.0 (TID 4)
[2026-02-02T10:26:17.541+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Registering RDD 23 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 4
[2026-02-02T10:26:17.544+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Got map stage job 5 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:17.545+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:17.545+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:17.545+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:17.545+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:17.545+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.7 KiB, free 1048.8 MiB)
[2026-02-02T10:26:17.560+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1048.8 MiB)
[2026-02-02T10:26:17.561+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 4ba644d6f495:37703 (size: 7.6 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:17.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:17.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[23] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:17.563+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2026-02-02T10:26:17.946+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO JDBCRDD: closed connection
[2026-02-02T10:26:17.971+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO Executor: Finished task 0.0 in stage 9.0 (TID 4). 2032 bytes result sent to driver
[2026-02-02T10:26:17.977+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 5) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:26:17.979+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 4) in 447 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:17.984+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO Executor: Running task 0.0 in stage 10.0 (TID 5)
[2026-02-02T10:26:17.985+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2026-02-02T10:26:17.985+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: ShuffleMapStage 9 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.468 s
[2026-02-02T10:26:17.985+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:17.986+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: running: Set(ShuffleMapStage 10)
[2026-02-02T10:26:17.986+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:17.986+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:17 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:18.090+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 9.489416 ms
[2026-02-02T10:26:18.106+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO JDBCRDD: closed connection
[2026-02-02T10:26:18.121+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO Executor: Finished task 0.0 in stage 10.0 (TID 5). 2032 bytes result sent to driver
[2026-02-02T10:26:18.124+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 5) in 148 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:18.125+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 4ba644d6f495:37703 in memory (size: 7.9 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:18.126+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: ShuffleMapStage 10 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.589 s
[2026-02-02T10:26:18.127+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:18.129+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: running: Set()
[2026-02-02T10:26:18.130+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:18.130+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:18.130+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2026-02-02T10:26:18.162+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO ShufflePartitionsUtil: For shuffle(3, 4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:26:18.286+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 43.069584 ms
[2026-02-02T10:26:18.311+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Registering RDD 30 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 5
[2026-02-02T10:26:18.312+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Got map stage job 6 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:18.313+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:18.313+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12, ShuffleMapStage 11)
[2026-02-02T10:26:18.315+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:18.316+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[30] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:18.325+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 67.9 KiB, free 1048.7 MiB)
[2026-02-02T10:26:18.366+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 4ba644d6f495:37703 in memory (size: 7.6 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:18.368+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1048.7 MiB)
[2026-02-02T10:26:18.370+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 4ba644d6f495:37703 (size: 30.1 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:18.370+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:18.370+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[30] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:18.372+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2026-02-02T10:26:18.373+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 6) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8379 bytes)
[2026-02-02T10:26:18.376+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO Executor: Running task 0.0 in stage 13.0 (TID 6)
[2026-02-02T10:26:18.424+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO ShuffleBlockFetcherIterator: Getting 1 (748.2 KiB) non-empty blocks including 1 (748.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:18.427+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2026-02-02T10:26:18.473+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO ShuffleBlockFetcherIterator: Getting 1 (1082.0 B) non-empty blocks including 1 (1082.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:18.476+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
[2026-02-02T10:26:18.650+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 165.212417 ms
[2026-02-02T10:26:18.692+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 22.305416 ms
[2026-02-02T10:26:18.722+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 17.749209 ms
[2026-02-02T10:26:18.824+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 22.874792 ms
[2026-02-02T10:26:18.857+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:18 INFO CodeGenerator: Code generated in 20.384709 ms
[2026-02-02T10:26:19.236+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO Executor: Finished task 0.0 in stage 13.0 (TID 6). 6897 bytes result sent to driver
[2026-02-02T10:26:19.248+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 6) in 877 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:19.249+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2026-02-02T10:26:19.252+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: ShuffleMapStage 13 (jdbc at NativeMethodAccessorImpl.java:0) finished in 0.928 s
[2026-02-02T10:26:19.254+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:26:19.255+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: running: Set()
[2026-02-02T10:26:19.256+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:26:19.256+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: failed: Set()
[2026-02-02T10:26:19.272+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:26:19.283+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2026-02-02T10:26:19.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO CodeGenerator: Code generated in 21.799875 ms
[2026-02-02T10:26:19.373+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0
[2026-02-02T10:26:19.376+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Got job 7 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:19.376+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Final stage: ResultStage 17 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:19.376+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2026-02-02T10:26:19.376+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:19.377+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:19.389+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 69.1 KiB, free 1048.6 MiB)
[2026-02-02T10:26:19.400+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 30.1 KiB, free 1048.6 MiB)
[2026-02-02T10:26:19.402+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 4ba644d6f495:37703 (size: 30.1 KiB, free: 1048.7 MiB)
[2026-02-02T10:26:19.404+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:19.406+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 4ba644d6f495:37703 in memory (size: 30.1 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:19.407+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:19.412+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2026-02-02T10:26:19.415+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 7) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8108 bytes)
[2026-02-02T10:26:19.416+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO Executor: Running task 0.0 in stage 17.0 (TID 7)
[2026-02-02T10:26:19.432+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO ShuffleBlockFetcherIterator: Getting 1 (6.8 KiB) non-empty blocks including 1 (6.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:26:19.433+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2026-02-02T10:26:19.479+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:19 INFO CodeGenerator: Code generated in 46.198417 ms
[2026-02-02T10:26:20.930+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:20 INFO CodeGenerator: Code generated in 46.407666 ms
[2026-02-02T10:26:21.096+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO Executor: Finished task 0.0 in stage 17.0 (TID 7). 7943 bytes result sent to driver
[2026-02-02T10:26:21.105+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 7) in 1697 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:26:21.107+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2026-02-02T10:26:21.113+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: ResultStage 17 (jdbc at NativeMethodAccessorImpl.java:0) finished in 1.731 s
[2026-02-02T10:26:21.114+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-02T10:26:21.116+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2026-02-02T10:26:21.117+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Job 7 finished: jdbc at NativeMethodAccessorImpl.java:0, took 1.740226 s
[2026-02-02T10:26:21.503+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO CodeGenerator: Code generated in 18.153209 ms
[2026-02-02T10:26:21.513+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Registering RDD 40 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 6
[2026-02-02T10:26:21.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Got map stage job 8 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:21.514+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Final stage: ShuffleMapStage 18 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:21.515+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:21.515+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:21.515+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[40] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:21.528+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 28.3 KiB, free 1048.7 MiB)
[2026-02-02T10:26:21.548+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1048.7 MiB)
[2026-02-02T10:26:21.549+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 4ba644d6f495:37703 (size: 13.6 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:21.551+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:21.551+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[40] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:21.552+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2026-02-02T10:26:21.556+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 8) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:26:21.558+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Registering RDD 42 (jdbc at NativeMethodAccessorImpl.java:0) as input to shuffle 7
[2026-02-02T10:26:21.560+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Got map stage job 9 (jdbc at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2026-02-02T10:26:21.562+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (jdbc at NativeMethodAccessorImpl.java:0)
[2026-02-02T10:26:21.566+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Parents of final stage: List()
[2026-02-02T10:26:21.566+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:26:21.566+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[42] at jdbc at NativeMethodAccessorImpl.java:0), which has no missing parents
[2026-02-02T10:26:21.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO Executor: Running task 0.0 in stage 18.0 (TID 8)
[2026-02-02T10:26:21.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 4ba644d6f495:37703 in memory (size: 30.1 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:21.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.3 KiB, free 1048.7 MiB)
[2026-02-02T10:26:21.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 1048.7 MiB)
[2026-02-02T10:26:21.567+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 4ba644d6f495:37703 (size: 7.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:26:21.571+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:26:21.572+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[42] at jdbc at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:26:21.573+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:21 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2026-02-02T10:26:33.266+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:33 INFO CodeGenerator: Code generated in 145.895875 ms
[2026-02-02T10:26:33.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:33 INFO CodeGenerator: Code generated in 20.236292 ms
[2026-02-02T10:26:33.430+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:33 INFO CodeGenerator: Code generated in 9.719167 ms
[2026-02-02T10:26:45.985+0000] {spark_submit.py:634} INFO - 26/02/02 10:26:45 INFO JDBCRDD: closed connection
[2026-02-02T10:27:00.761+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO Executor: Finished task 0.0 in stage 18.0 (TID 8). 2341 bytes result sent to driver
[2026-02-02T10:27:00.791+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 9) (4ba644d6f495, executor driver, partition 0, PROCESS_LOCAL, 7943 bytes)
[2026-02-02T10:27:00.798+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 8) in 39245 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:27:00.799+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2026-02-02T10:27:00.800+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO Executor: Running task 0.0 in stage 19.0 (TID 9)
[2026-02-02T10:27:00.825+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO DAGScheduler: ShuffleMapStage 18 (jdbc at NativeMethodAccessorImpl.java:0) finished in 39.299 s
[2026-02-02T10:27:00.828+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:27:00.829+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO DAGScheduler: running: Set(ShuffleMapStage 19)
[2026-02-02T10:27:00.829+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:27:00.830+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:00 INFO DAGScheduler: failed: Set()
[2026-02-02T10:27:01.433+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO JDBCRDD: closed connection
[2026-02-02T10:27:01.476+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO Executor: Finished task 0.0 in stage 19.0 (TID 9). 2032 bytes result sent to driver
[2026-02-02T10:27:01.493+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 9) in 705 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:27:01.500+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2026-02-02T10:27:01.504+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO DAGScheduler: ShuffleMapStage 19 (jdbc at NativeMethodAccessorImpl.java:0) finished in 39.942 s
[2026-02-02T10:27:01.510+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO DAGScheduler: looking for newly runnable stages
[2026-02-02T10:27:01.512+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO DAGScheduler: running: Set()
[2026-02-02T10:27:01.513+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO DAGScheduler: waiting: Set()
[2026-02-02T10:27:01.515+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO DAGScheduler: failed: Set()
[2026-02-02T10:27:01.538+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2026-02-02T10:27:01.933+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:01 INFO CodeGenerator: Code generated in 199.850417 ms
[2026-02-02T10:27:02.039+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 4ba644d6f495:37703 in memory (size: 13.6 KiB, free: 1048.8 MiB)
[2026-02-02T10:27:02.282+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2026-02-02T10:27:02.286+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Got job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2026-02-02T10:27:02.288+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Final stage: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2026-02-02T10:27:02.290+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
[2026-02-02T10:27:02.291+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Missing parents: List()
[2026-02-02T10:27:02.291+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2026-02-02T10:27:02.301+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 39.7 KiB, free 1048.7 MiB)
[2026-02-02T10:27:02.320+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 1048.7 MiB)
[2026-02-02T10:27:02.320+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 4ba644d6f495:37703 (size: 18.5 KiB, free: 1048.8 MiB)
[2026-02-02T10:27:02.321+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580
[2026-02-02T10:27:02.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2026-02-02T10:27:02.323+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2026-02-02T10:27:02.326+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 10) (4ba644d6f495, executor driver, partition 0, NODE_LOCAL, 8108 bytes)
[2026-02-02T10:27:02.329+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO Executor: Running task 0.0 in stage 21.0 (TID 10)
[2026-02-02T10:27:02.428+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO ShuffleBlockFetcherIterator: Getting 1 (53.1 KiB) non-empty blocks including 1 (53.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2026-02-02T10:27:02.431+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
[2026-02-02T10:27:02.536+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO CodeGenerator: Code generated in 38.317917 ms
[2026-02-02T10:27:02.610+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO CodeGenerator: Code generated in 8.80675 ms
[2026-02-02T10:27:02.619+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO CodeGenerator: Code generated in 4.05625 ms
[2026-02-02T10:27:02.632+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO CodeGenerator: Code generated in 5.083042 ms
[2026-02-02T10:27:02.660+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO CodeGenerator: Code generated in 29.272 ms
[2026-02-02T10:27:02.726+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO Executor: Finished task 0.0 in stage 21.0 (TID 10). 4773 bytes result sent to driver
[2026-02-02T10:27:02.736+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 10) in 411 ms on 4ba644d6f495 (executor driver) (1/1)
[2026-02-02T10:27:02.737+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2026-02-02T10:27:02.738+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: ResultStage 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.442 s
[2026-02-02T10:27:02.739+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2026-02-02T10:27:02.740+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2026-02-02T10:27:02.741+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Job 10 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.457927 s
[2026-02-02T10:27:02.786+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 24.0 B, free 1048.7 MiB)
[2026-02-02T10:27:02.803+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 120.0 B, free 1048.7 MiB)
[2026-02-02T10:27:02.804+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 4ba644d6f495:37703 (size: 120.0 B, free: 1048.8 MiB)
[2026-02-02T10:27:02.806+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2026-02-02T10:27:02.962+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO SparkContext: Starting job: jdbc at NativeMethodAccessorImpl.java:0
[2026-02-02T10:27:02.964+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:02 INFO DAGScheduler: Job 11 finished: jdbc at NativeMethodAccessorImpl.java:0, took 0.000634 s
[2026-02-02T10:27:03.159+0000] {spark_submit.py:634} INFO -  Pipeline finished for 2026-02-02
[2026-02-02T10:27:03.182+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2026-02-02T10:27:03.256+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO SparkUI: Stopped Spark web UI at http://4ba644d6f495:4040
[2026-02-02T10:27:03.297+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2026-02-02T10:27:03.382+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO MemoryStore: MemoryStore cleared
[2026-02-02T10:27:03.384+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO BlockManager: BlockManager stopped
[2026-02-02T10:27:03.391+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2026-02-02T10:27:03.402+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2026-02-02T10:27:03.432+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO SparkContext: Successfully stopped SparkContext
[2026-02-02T10:27:03.936+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO ShutdownHookManager: Shutdown hook called
[2026-02-02T10:27:03.944+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb/pyspark-5ce2eb82-9d1d-40c9-adf8-ffc6a7c0ad2c
[2026-02-02T10:27:03.968+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3995e7a-2e8a-4e6a-b908-c3c53740dfbb
[2026-02-02T10:27:03.984+0000] {spark_submit.py:634} INFO - 26/02/02 10:27:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-f59fb9b1-0bd7-4514-ad2f-5ec465549bfd
[2026-02-02T10:27:04.629+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=daily_order_analytics, task_id=run_batch_pipeline, execution_date=20260202T102538, start_date=20260202T102557, end_date=20260202T102704
[2026-02-02T10:27:04.719+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2026-02-02T10:27:04.853+0000] {taskinstance.py:3280} INFO - 1 downstream tasks scheduled from follow-on schedule check
