x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgresql:5432/airflow
    AIRFLOW_CONN_POSTGRES_DEFAULT: 'postgres://postgres:postgres@postgresql:5432/postgres'
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ''
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    postgresql:
      condition: service_healthy

services:
  # PostgreSQL for both Airflow metadata and data warehouse
  postgresql:
    image: postgres:17-alpine
    platform: linux/arm64
    ports:
      - '5432:5432'
    volumes:
      - postgresql_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  # Kafka (from original setup)
  kafka:
    image: apache/kafka:latest
    platform: linux/arm64
    ports:
      - "9092:9092"
      - "9094:9094"
    volumes:
      - "kafka_data:/bitnami"
    environment:
      - KAFKA_NODE_ID=0
      - KAFKA_PROCESS_ROLES=controller,broker
      - KAFKA_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    restart: always

  # Kafka UI (from original setup)
  kafka-ui:
    container_name: kafka-ui
    platform: linux/arm64
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      - KAFKA_CLUSTERS_0_NAME=Local-Kafka         
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - DYNAMIC_CONFIG_ENABLED=true
    restart: always

  # Airflow webserver
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # Airflow scheduler
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    volumes:
      - ./dags:/opt/airflow/dags
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  # Airflow initialization
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # 1. Wait for Postgres
        until pg_isready -h postgresql -U postgres; do
          echo "Waiting for Postgres..."
          sleep 2
        done

        export PGPASSWORD='postgres'
        
        # 2. Check and Create DB (Use $$ for internal bash variables)
        RESULT=$$(psql -h postgresql -U postgres -tAc "SELECT 1 FROM pg_database WHERE datname='airflow'")
        if [ "$$RESULT" != "1" ]; then
          echo "Creating database 'airflow'..."
          psql -h postgresql -U postgres -c "CREATE DATABASE airflow"
        fi
        
        # 3. Migrate and Create User
        airflow db migrate
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
        
        # 4. Add Connection for Spark
        airflow connections add 'spark_default' --conn-type 'spark' --conn-host 'local' || true
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      # Ensure root can import packages installed in the airflow user's ~/.local
      PYTHONPATH: /home/airflow/.local/lib/python3.11/site-packages
    user: "0:0"

volumes:
  kafka_data:
    driver: local
  postgresql_data:
    driver: local