# Problem 2 â€“ Streaming Pipeline

## Overview
This project shows a simple real-time data pipeline using **Kafka** and **Spark Streaming**.

Order events are generated by a small data simulator and sent to a Kafka topic.
A streaming application then reads these events, processes them in real time,
and continuously calculates order counts per product using time windows.

The results are printed to the console as new events arrive.


## Project Flow
The streaming pipeline works as follows:
1. **Order Data Simulator**
   - Generates mock order events in JSON format
   - Sends events to a Kafka topic named `orders`

2. **Kafka**
   - Acts as a message broker
   - Buffers order events between producer and consumer

3. **Spark Structured Streaming**
   - Reads order events from Kafka
   - Uses event-time processing with 1-minute tumbling windows
   - Aggregates order count per product
   - Outputs results to the console continuously

## Example Event
Each order event looks like this

```json
{
  "order_id": "ORD-1",
  "order_timestamp": "2026-02-04 12:45:10",
  "user_id": "USER-0001",
  "product_id": "PROD-00005",
  "quantity": 2,
  "status": "CREATED"
}
```


## **How to Run**

#### Step 1: Start Kafka
Start Kafka and related services using Docker

`docker compose up -d`

Make sure the Kafka topic orders is available before running the pipeline.

#### Step 2: Run Order Data Simulator
Open a new terminal and start the data simulator

`python3 data_simulator.py`

This script will continuously generate and send order events to Kafka.

#### Step 3: Run Streaming Pipeline
In another terminal, start the Spark Structured Streaming application

`python3 streaming_pipeline.py --output console`

The application will consume events from Kafka and print aggregated results to the console in real time.