# Problem 2 â€“ Streaming Pipeline

## Overview
This project shows a real-time data pipeline using **Kafka** and **Spark Streaming**.

Order events are generated by a small data simulator and sent to a **Kafka topic.**
A streaming application then reads these events, processes them in real time,
and continuously calculates order counts per product using time windows. The results are printed to the console as new events arrive.


## Project Flow
1. **Order Data Simulator**
   - Generates mock order events in JSON format
   - Sends events to a Kafka topic named `orders`

2. **Kafka**
   - Acts as a message broker
   - Buffers order events between producer and consumer

3. **Spark Structured Streaming**
   - Reads order events from Kafka
   - Uses event-time processing with 1-minute tumbling windows
   - Aggregates order count per product
   - Outputs results to the console continuously

## Example Event
Each order event looks like this

```json
{
  "order_id": "ORD-1",
  "order_timestamp": "2026-02-04 12:45:10",
  "user_id": "USER-0001",
  "product_id": "PROD-00005",
  "quantity": 2,
  "status": "CREATED"
}
```
