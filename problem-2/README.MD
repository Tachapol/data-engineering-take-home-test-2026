# Streaming Pipeline

## Overview

This project demonstrates a simple real-time data pipeline using Kafka and Spark Structured Streaming.
Order events are first generated by a data simulator and sent to a Kafka topic.
The streaming application consumes these events and aggregates order counts per product using 1-minute tumbling windows.
Results are continuously updated and printed to the console as new events arrive.

---

## Project Flow

1. **Order Data Simulator**
   - Generates mock order events in JSON format
   - Sends events to Kafka topic `orders` in real time

2. **Kafka**
   - Acts as a message broker between producer and consumer

3. **Spark Structured Streaming**
   - Reads order events from Kafka
   - Applies event-time windowing (1 minute)
   - Aggregates order count per product
   - Outputs results to the console

---

## How to Run

### 1. Start Kafka

Start Kafka and related services using Docker:

```bash
docker compose up -d
```

Make sure the Kafka topic orders is available.

2. Run Order Data Simulator
In the first terminal, start the data simulator to produce order events:

```python3 data_simulator.py```
The simulator will continuously send order messages to Kafka.

Example event:
```{"order_id":"ORD-1","order_timestamp":"2026-02-04 12:45:10","user_id":"USER-0001","product_id":"PROD-00005","quantity":2,"status":"CREATED"}```

3. Run Streaming Pipeline
In another terminal, start the Spark Structured Streaming application:

```python3 streaming_pipeline.py --output console```
The application will consume events from Kafka and print aggregated results in real time.